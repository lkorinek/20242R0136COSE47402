{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DMz22cJwhDWz",
      "metadata": {
        "id": "DMz22cJwhDWz"
      },
      "outputs": [],
      "source": [
        "!pip install datasets peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kjN5TG2d3Fto",
      "metadata": {
        "id": "kjN5TG2d3Fto"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Log in using your token\n",
        "login(userdata.get('hugging_face_token'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bOnNte8m5uKo",
      "metadata": {
        "id": "bOnNte8m5uKo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "root = \"/content/drive/\"\n",
        "drive.mount(root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vy3AQHpe5qHb",
      "metadata": {
        "id": "vy3AQHpe5qHb"
      },
      "outputs": [],
      "source": [
        "import json, os, sys, re, json\n",
        "\n",
        "path = os.path.join(root, \"My Drive/Colab Notebooks/COSE474\")\n",
        "os.makedirs(path, exist_ok=True)\n",
        "\n",
        "od_path = os.path.join(path, \"Rust_Code_Generation\")\n",
        "os.makedirs(od_path, exist_ok=True)\n",
        "\n",
        "%cd \"{od_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbEs9JANJT-X",
      "metadata": {
        "id": "cbEs9JANJT-X"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
        ")\n",
        "from peft import get_peft_config, get_peft_model, PromptTuningConfig, TaskType\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fbfca2",
      "metadata": {
        "id": "71fbfca2"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name_or_path = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer_name_or_path = model_name_or_path\n",
        "prompt_tuning_init_text = \"You are an expert Rust programmer. Use RUST_END to delimit the rust function.\"\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Configure PEFT Prompt Tuning\n",
        "peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    prompt_tuning_init_text=prompt_tuning_init_text,\n",
        "    num_virtual_tokens=len(tokenizer(prompt_tuning_init_text)[\"input_ids\"]),\n",
        "    tokenizer_name_or_path=model_name_or_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AJLO3bbB1sho",
      "metadata": {
        "id": "AJLO3bbB1sho"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"bigcode/humanevalpack\", \"rust\")\n",
        "\n",
        "# Define the split ratios\n",
        "train_ratio = 0.16\n",
        "valid_ratio = 0.04\n",
        "test_ratio = 0.8\n",
        "\n",
        "# First split: train + (valid + test)\n",
        "split_dataset = dataset[\"test\"].train_test_split(test_size=1 - train_ratio, seed=42)\n",
        "\n",
        "# Second split: valid + test\n",
        "valid_test_split = split_dataset[\"test\"].train_test_split(test_size=test_ratio / (valid_ratio + test_ratio), seed=42)\n",
        "\n",
        "# Combine into DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": split_dataset[\"train\"],\n",
        "    \"valid\": valid_test_split[\"train\"],\n",
        "    \"test\": valid_test_split[\"test\"]\n",
        "})\n",
        "\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BqO9gkiE_yne",
      "metadata": {
        "id": "BqO9gkiE_yne"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
        "\n",
        "# Load the extra dataset from a JSONL file\n",
        "extra_data = []\n",
        "with open(\"extra_train_data.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        extra_data.append(json.loads(line.strip()))\n",
        "\n",
        "# Split the list of dictionaries into 8:2 train:valid\n",
        "extra_train, extra_valid = train_test_split(extra_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the split lists back into Hugging Face Datasets\n",
        "extra_train = Dataset.from_list(extra_train)\n",
        "extra_valid = Dataset.from_list(extra_valid)\n",
        "\n",
        "dataset[\"train\"] = concatenate_datasets([dataset[\"train\"], extra_train])\n",
        "dataset[\"valid\"] = concatenate_datasets([dataset[\"valid\"], extra_valid])\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ybabDir_W58b",
      "metadata": {
        "id": "ybabDir_W58b"
      },
      "outputs": [],
      "source": [
        "def extract_number(x):\n",
        "    try:\n",
        "        return int(x.split('/')[1])\n",
        "    except ValueError:\n",
        "        return float('inf')\n",
        "\n",
        "print(sorted(dataset[\"train\"][\"task_id\"], key=extract_number))\n",
        "print(sorted(dataset[\"valid\"][\"task_id\"], key=extract_number))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pyIOKphizBGd",
      "metadata": {
        "id": "pyIOKphizBGd"
      },
      "outputs": [],
      "source": [
        "def remove_before_fn(s: str) -> str:\n",
        "    pos = s.find(\"fn\")\n",
        "    if pos != -1:\n",
        "        return s[pos:]\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4NhKVwtM5GmR",
      "metadata": {
        "collapsed": true,
        "id": "4NhKVwtM5GmR"
      },
      "outputs": [],
      "source": [
        "# Preprocessing Function\n",
        "max_length = 240\n",
        "text_column = \"instruction\"\n",
        "label_column = \"canonical_solution\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    batch_size = len(examples[text_column])\n",
        "    inputs = [f\"{x}\\nRUST_BEGIN\\n\" for x in examples[text_column]]\n",
        "    targets = [str(x) for x in examples[label_column]]\n",
        "    # targets = [f\"{remove_before_fn(declaration.strip())}\\n    {canonical_solution.strip()}\\nRUST_END\\n\" for declaration, canonical_solution in zip(examples['declaration'], examples['canonical_solution'])]\n",
        "    targets = [f\"{declaration.strip()}\\n    {canonical_solution.strip()}\\nRUST_END\\n\" for declaration, canonical_solution in zip(examples['declaration'], examples['canonical_solution'])]\n",
        "    model_inputs = tokenizer(inputs)\n",
        "    labels = tokenizer(targets, add_special_tokens=False)\n",
        "    for i in range(batch_size):\n",
        "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
        "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
        "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
        "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
        "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
        "        label_input_ids = labels[\"input_ids\"][i]\n",
        "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
        "            max_length - len(sample_input_ids)\n",
        "        ) + sample_input_ids\n",
        "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
        "            \"attention_mask\"\n",
        "        ][i]\n",
        "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
        "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
        "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
        "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "processed_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "train_dataset = processed_datasets[\"train\"]\n",
        "eval_dataset = processed_datasets[\"valid\"]\n",
        "print(train_dataset, eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pwa1X_Qr7DIG",
      "metadata": {
        "id": "pwa1X_Qr7DIG"
      },
      "outputs": [],
      "source": [
        "# DataLoaders\n",
        "batch_size = 8\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset, collate_fn=default_data_collator, batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zIQhbPoPSszt",
      "metadata": {
        "id": "zIQhbPoPSszt"
      },
      "outputs": [],
      "source": [
        "# Model Initialization\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.to(device)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe12d4d3",
      "metadata": {
        "id": "fe12d4d3"
      },
      "outputs": [],
      "source": [
        "# Optimizer and Learning Rate Scheduler\n",
        "lr = 3e-2\n",
        "num_epochs = 15\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=len(train_dataloader) * num_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641b21fe",
      "metadata": {
        "id": "641b21fe"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        eval_loss += outputs.loss.item()\n",
        "\n",
        "    eval_loss /= len(eval_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Evaluation Loss: {eval_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jXtVkmR-aBsM",
      "metadata": {
        "id": "jXtVkmR-aBsM"
      },
      "outputs": [],
      "source": [
        "output_dir = \"./rust_code_generation_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accc5012",
      "metadata": {
        "id": "accc5012"
      },
      "outputs": [],
      "source": [
        "# Save the Model\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uUyfnhkEynIX",
      "metadata": {
        "id": "uUyfnhkEynIX"
      },
      "outputs": [],
      "source": [
        "model_dir = \"./test_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9476e1",
      "metadata": {
        "id": "4d9476e1"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "config = PeftConfig.from_pretrained(model_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
        "model = PeftModel.from_pretrained(model, model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UQNrBng8AWzO",
      "metadata": {
        "id": "UQNrBng8AWzO"
      },
      "outputs": [],
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class StopOnRUSTEND(StoppingCriteria):\n",
        "    def __init__(self, stop_sequence: str, tokenizer, max_occurrences: int = 2):\n",
        "        self.stop_sequence = stop_sequence\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_occurrences = max_occurrences\n",
        "        self.current_count = 0\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        # Convert the current generated tokens back to text\n",
        "        text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "        # Count occurrences of 'RUST_END' in the generated text\n",
        "        self.current_count = text.count(self.stop_sequence)\n",
        "\n",
        "        # Stop if the 'RUST_END' sequence has appeared max_occurrences times\n",
        "        return self.current_count >= self.max_occurrences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24041ee1",
      "metadata": {
        "id": "24041ee1"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QGLON2HRkvZQ",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QGLON2HRkvZQ"
      },
      "outputs": [],
      "source": [
        "for i in range(0, 2):\n",
        "    prompt = (\n",
        "        f\"{dataset['test'][i]['instruction']}\\nRUST_BEGIN\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=512,\n",
        "            eos_token_id=3,\n",
        "            stopping_criteria = StoppingCriteriaList([StopOnRUSTEND(\"RUST_END\", tokenizer)])\n",
        "        )\n",
        "\n",
        "        # Decode the generated tokens back into text\n",
        "        generated_code = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n",
        "        # Access the first element of the generated_code list (which is the string)\n",
        "        generated_code_str = generated_code[0]\n",
        "        matches = re.findall(r'RUST_BEGIN\\s*(.*?)\\s*RUST_END', generated_code_str, re.DOTALL)\n",
        "        rust_code = matches[0].strip() if len(matches) > 0 else \"\"  # Extract the code between RUST_BEGIN and RUST_END\n",
        "\n",
        "        print(\"=============== NEXT ===============\")\n",
        "        print()\n",
        "        print(rust_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wHkYeyPzZJVT",
      "metadata": {
        "id": "wHkYeyPzZJVT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import DatasetDict\n",
        "\n",
        "\n",
        "# Extract task IDs and corresponding indices\n",
        "task_ids_and_indices = [(dataset['test'][i]['task_id'], i) for i in range(len(dataset['test']))]\n",
        "\n",
        "# Sort based on the numerical part of the task ID\n",
        "sorted_task_ids_and_indices = sorted(\n",
        "    task_ids_and_indices,\n",
        "    key=lambda item: int(re.findall(r'\\d+', item[0])[0]),\n",
        "    reverse=False\n",
        ")\n",
        "\n",
        "# Get the sorted indices\n",
        "sorted_indices = [item[1] for item in sorted_task_ids_and_indices]\n",
        "\n",
        "# Create a new Dataset with the sorted order\n",
        "sorted_dataset_test = dataset['test'].select(sorted_indices)\n",
        "\n",
        "# Update the DatasetDict\n",
        "dataset = DatasetDict({\"train\": dataset[\"train\"], \"valid\": dataset[\"valid\"], \"test\": sorted_dataset_test})\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5KiOLJdzkY6b",
      "metadata": {
        "id": "5KiOLJdzkY6b"
      },
      "outputs": [],
      "source": [
        "class RustCodeGenerator:\n",
        "    def __init__(self, output_filename, tokenizer, model, num_samples = 164, total_samples = 164, start_idx = 0, max_new_tokens = 300):\n",
        "        self.output_filename = output_filename\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.num_samples = num_samples if num_samples <= total_samples else total_samples\n",
        "        self.total_samples = total_samples\n",
        "        self.start_idx = start_idx\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "    def print_progress_bar(self, current, total, bar_length=50):\n",
        "        # Calculate the percentage of completion\n",
        "        percent = (current / total) * 100\n",
        "        # Determine the number of \"#\" characters in the bar based on the percentage\n",
        "        filled_length = int(bar_length * current // total)\n",
        "        bar = '#' * filled_length + '-' * (bar_length - filled_length)\n",
        "\n",
        "        # Use '\\r' to overwrite the current line and display the loading bar\n",
        "        sys.stdout.write(f'\\rProgress: |{bar}| {percent:.2f}% ({current}/{total})')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def process_line(self, idx):\n",
        "        prompt = f'{dataset[\"test\"][idx][\"instruction\"]}\\nRUST_BEGIN\\n'\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        stop_sequence = \"RUST_END\"\n",
        "        stopping_criteria = StoppingCriteriaList([StopOnRUSTEND(stop_sequence, self.tokenizer)])\n",
        "\n",
        "        attempt = 0\n",
        "        max_attempts = 5\n",
        "        rust_code = \"\"\n",
        "\n",
        "        with torch.no_grad():\n",
        "          inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "          while attempt < max_attempts:\n",
        "              # Configure generation parameters\n",
        "              generation_kwargs = {\n",
        "                  \"num_return_sequences\": 1,\n",
        "                  \"max_new_tokens\": self.max_new_tokens,\n",
        "                  \"stopping_criteria\": stopping_criteria,\n",
        "                  \"return_dict_in_generate\": True,\n",
        "                  \"output_scores\": True,\n",
        "                  \"input_ids\": inputs[\"input_ids\"],\n",
        "                  \"attention_mask\": inputs[\"attention_mask\"],\n",
        "                  \"eos_token_id\": 3,\n",
        "              }\n",
        "\n",
        "              # First attempt: Deterministic decoding\n",
        "              if attempt == 0:\n",
        "                  generation_kwargs.update({\n",
        "                      \"do_sample\": False,\n",
        "                      \"temperature\": None,\n",
        "                      \"top_p\": None,\n",
        "                  })\n",
        "              else:  # Subsequent attempts: Sampling with temperature\n",
        "                 generation_kwargs.update({\n",
        "                    \"do_sample\": True,\n",
        "                    \"temperature\": 0.5,\n",
        "                    \"top_p\": 0.9,\n",
        "                    \"max_tokens\": 512,\n",
        "                    \"top_k\": 50,\n",
        "              })\n",
        "\n",
        "              outputs = self.model.generate(**generation_kwargs)\n",
        "\n",
        "              # Decode the generated tokens back into text\n",
        "              # generated_code = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n",
        "              generated_code = self.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
        "\n",
        "              generated_code_str = generated_code[0]\n",
        "              matches = re.findall(r'RUST_BEGIN\\s*(.*?)\\s*RUST_END', generated_code_str, re.DOTALL)\n",
        "\n",
        "              matches_in_prompt = re.findall(r'RUST_BEGIN\\s*(.*?)\\s*RUST_END', prompt, re.DOTALL)\n",
        "              rust_code = matches[len(matches_in_prompt)].strip() if len(matches) > len(matches_in_prompt) else \"\"  # Extract the code between RUST_BEGIN and RUST_END\n",
        "\n",
        "              # Break if valid code is generated\n",
        "              if len(rust_code) > 0:\n",
        "                  print(f\"Attempt {attempt}: Generated code length is {len(rust_code)}\")\n",
        "                  break\n",
        "\n",
        "              attempt += 1\n",
        "              print(f\"Attempt {attempt}: Generated code length is 0. Retrying...\")\n",
        "\n",
        "          return {\n",
        "              'task_id': dataset[\"test\"][idx][\"task_id\"],\n",
        "              'instruction': dataset[\"test\"][idx][\"instruction\"],\n",
        "              'generated_code': rust_code if len(rust_code) > 0 else \"No valid code generated.\",\n",
        "              'test': dataset[\"test\"][idx][\"test\"],\n",
        "              'attempt': attempt\n",
        "          }\n",
        "\n",
        "    def save_output(self, generated_codes):\n",
        "        with open(self.output_filename, \"w\") as file:\n",
        "            json.dump(generated_codes, file, indent=0)\n",
        "\n",
        "    def process(self):\n",
        "        generated_codes = []\n",
        "\n",
        "        # Load existing data if the file exists\n",
        "        if os.path.exists(self.output_filename) and os.path.getsize(self.output_filename) > 0:\n",
        "            with open(self.output_filename, \"r\") as file:\n",
        "                generated_codes = json.load(file)\n",
        "\n",
        "        for i in range(self.start_idx, len(dataset[\"test\"])):\n",
        "          generated_code_data = self.process_line(i)\n",
        "          generated_codes.append(generated_code_data)\n",
        "\n",
        "          self.save_output(generated_codes)\n",
        "          self.print_progress_bar(i, len(dataset[\"test\"]))\n",
        "\n",
        "\n",
        "        self.print_progress_bar(self.num_samples, self.num_samples)\n",
        "\n",
        "\n",
        "    def print_example_code(self, idx=0):\n",
        "\n",
        "        if os.path.exists(self.output_filename) and os.path.getsize(self.output_filename) > 0:\n",
        "            with open(self.output_filename, \"r\") as file:\n",
        "                generated_codes = json.load(file)\n",
        "                if generated_codes:\n",
        "                    # Take the first entry\n",
        "                    try:\n",
        "                      generated_code_data = generated_codes[idx]\n",
        "                    except IndexError:\n",
        "                      print(\"Index out of range.\")\n",
        "                      return\n",
        "\n",
        "                    task_id = generated_code_data.get(\"task_id\", \"No task available\")\n",
        "                    prompt = generated_code_data.get(\"instruction\", \"No prompt available\")\n",
        "                    code = generated_code_data.get(\"generated_code\", \"No code available\")\n",
        "\n",
        "                    print(\"Task \" + task_id + \":\")\n",
        "                    print(\"Example Prompt:\")\n",
        "                    print(prompt)\n",
        "                    print(\"\\nGenerated Code:\")\n",
        "                    print(code)\n",
        "                else:\n",
        "                    print(\"No data available in the output file.\")\n",
        "        else:\n",
        "            print(\"Output file does not exist or is empty.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RYycuGHcmyeh",
      "metadata": {
        "id": "RYycuGHcmyeh"
      },
      "outputs": [],
      "source": [
        "output_filename = \"output_tuned.json\"\n",
        "\n",
        "if os.path.exists(output_filename):\n",
        "  os.remove(output_filename)\n",
        "\n",
        "generator = RustCodeGenerator(\n",
        "    output_filename,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    max_new_tokens = 1024,\n",
        ")\n",
        "\n",
        "generator.process()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}