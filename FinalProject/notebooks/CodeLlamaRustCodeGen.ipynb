{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xZVuEZS2cD2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "root = \"/content/drive/\"\n",
        "drive.mount(root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvys6LVDqLSR"
      },
      "outputs": [],
      "source": [
        "import json, os, sys, re, json\n",
        "\n",
        "path = os.path.join(root, \"My Drive/Colab Notebooks/COSE474\")\n",
        "os.makedirs(path, exist_ok=True)\n",
        "\n",
        "od_path = os.path.join(path, \"Rust_Code_Generation\")\n",
        "os.makedirs(od_path, exist_ok=True)\n",
        "\n",
        "%cd \"{od_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3ZH4kfm0g5_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHLHTRai8cWj"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "with open(\"token_file.txt\", \"r\") as file:\n",
        "    token = file.read().strip()\n",
        "\n",
        "# Log in using your token\n",
        "login(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lRr9zZY7cvS"
      },
      "outputs": [],
      "source": [
        "# model_name = \"meta-llama/CodeLlama-13b-Instruct-hf\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the GPU (or CPU if no GPU available)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "IK8fw6-UHey8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = next(model.parameters()).device\n",
        "print(f\"The model is loaded on: {device}\")"
      ],
      "metadata": {
        "id": "_BhvXQm1XJ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqprBjCD4pUR"
      },
      "outputs": [],
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class StopOnRUSTEND(StoppingCriteria):\n",
        "    def __init__(self, stop_sequence: str, tokenizer, max_occurrences: int = 4):\n",
        "        self.stop_sequence = stop_sequence\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_occurrences = max_occurrences\n",
        "        self.current_count = 0\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        # Convert the current generated tokens back to text\n",
        "        text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "        # Count occurrences of 'RUST_END' in the generated text\n",
        "        self.current_count = text.count(self.stop_sequence)\n",
        "\n",
        "        # Stop if the 'RUST_END' sequence has appeared max_occurrences times\n",
        "        return self.current_count >= self.max_occurrences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL89WkRvXc50"
      },
      "outputs": [],
      "source": [
        "class RustCodeGenerator:\n",
        "    def __init__(self, input_filename, output_filename, tokenizer, model, num_samples = 164, total_samples = 164, start_idx = 0, max_new_tokens = 300):\n",
        "        self.input_filename = input_filename\n",
        "        self.output_filename = output_filename\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.num_samples = num_samples if num_samples <= total_samples else total_samples\n",
        "        self.total_samples = total_samples\n",
        "        self.start_idx = start_idx\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "    def print_progress_bar(self, current, total, bar_length=50):\n",
        "        # Calculate the percentage of completion\n",
        "        percent = (current / total) * 100\n",
        "        # Determine the number of \"#\" characters in the bar based on the percentage\n",
        "        filled_length = int(bar_length * current // total)\n",
        "        bar = '#' * filled_length + '-' * (bar_length - filled_length)\n",
        "\n",
        "        # Use '\\r' to overwrite the current line and display the loading bar\n",
        "        sys.stdout.write(f'\\rProgress: |{bar}| {percent:.2f}% ({current}/{total})')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def process_line(self, line, row):\n",
        "\n",
        "        one_shot = (\n",
        "            f\"You are an expert Rust programmer. Write a Rust function `add_two_numbers(a: i32, b: i32) -> i32` that returns the sum of two integers.\"\n",
        "            \"In your response, use RUST_BEGIN and RUST_END to delimit the rust function.\"\n",
        "            \"RUST_BEGIN\"\n",
        "            \"fn add_two_numbers(a: i32, b: i32) -> i32 {\"\n",
        "            \"    a + b\"\n",
        "            \"}\"\n",
        "            \"RUST_END\"\n",
        "            \"\"\n",
        "        )\n",
        "\n",
        "        prompt = (\n",
        "            f\"You are an expert Rust programmer. {row['instruction']}\"\n",
        "            \"In your response, use RUST_BEGIN and RUST_END to delimit the rust function.\\nRUST_BEGIN\\n\"\n",
        "        )\n",
        "        prompt = one_shot + prompt\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        stop_sequence = \"RUST_END\"\n",
        "        stopping_criteria = StoppingCriteriaList([StopOnRUSTEND(stop_sequence, self.tokenizer)])\n",
        "\n",
        "        attempt = 0\n",
        "        max_attempts = 10\n",
        "        rust_code = \"\"\n",
        "\n",
        "        while attempt < max_attempts:\n",
        "            # Configure generation parameters\n",
        "            generation_kwargs = {\n",
        "                \"num_return_sequences\": 1,\n",
        "                \"max_new_tokens\": self.max_new_tokens,\n",
        "                \"stopping_criteria\": stopping_criteria,\n",
        "                \"return_dict_in_generate\": True,\n",
        "                \"output_scores\": True,\n",
        "            }\n",
        "\n",
        "            # First attempt: Deterministic decoding\n",
        "            if attempt == 0:\n",
        "                generation_kwargs.update({\n",
        "                    \"do_sample\": False,\n",
        "                    \"temperature\": None,\n",
        "                    \"top_p\": None,\n",
        "                })\n",
        "            else:  # Subsequent attempts: Sampling with temperature\n",
        "                generation_kwargs.update({\n",
        "                    \"do_sample\": True,\n",
        "                    \"temperature\": 0.3,\n",
        "                    \"top_p\": 0.9,\n",
        "                })\n",
        "\n",
        "            outputs = self.model.generate(**inputs, **generation_kwargs)\n",
        "\n",
        "            # Decode and extract generated code\n",
        "            decoded_texts = self.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
        "            generated_code = \"\".join(decoded_texts)\n",
        "            matches = re.findall(r'RUST_BEGIN\\s*(.*?)\\s*RUST_END', generated_code, re.DOTALL)\n",
        "\n",
        "            matches_in_prompt = re.findall(r'RUST_BEGIN\\s*(.*?)\\s*RUST_END', prompt, re.DOTALL)\n",
        "            rust_code = matches[len(matches_in_prompt)].strip() if len(matches) > len(matches_in_prompt) else \"\"  # Extract the code between RUST_BEGIN and RUST_END\n",
        "\n",
        "            # Break if valid code is generated\n",
        "            if len(rust_code) > 0:\n",
        "                print(f\"Attempt {attempt}: Generated code length is {len(rust_code)}\")\n",
        "                break\n",
        "\n",
        "            attempt += 1\n",
        "            print(f\"Attempt {attempt}: Generated code length is 0. Retrying...\")\n",
        "\n",
        "        return {\n",
        "            'task_id': row[\"task_id\"],\n",
        "            'instruction': row[\"instruction\"],\n",
        "            'generated_code': rust_code if len(rust_code) > 0 else \"No valid code generated.\",\n",
        "            'test': row[\"test\"],\n",
        "            'attempt': attempt\n",
        "        }\n",
        "\n",
        "    def save_output(self, generated_codes):\n",
        "        with open(self.output_filename, \"w\") as file:\n",
        "            json.dump(generated_codes, file, indent=0)\n",
        "\n",
        "    def process(self):\n",
        "        generated_codes = []\n",
        "\n",
        "        # Load existing data if the file exists\n",
        "        if os.path.exists(self.output_filename) and os.path.getsize(self.output_filename) > 0:\n",
        "            with open(self.output_filename, \"r\") as file:\n",
        "                generated_codes = json.load(file)\n",
        "\n",
        "        with open(self.input_filename, \"r\") as f:\n",
        "            for i in range(self.total_samples):\n",
        "                if i < self.start_idx:\n",
        "                    f.readline().strip()  # Skip lines until start_idx\n",
        "                    continue\n",
        "\n",
        "                line = f.readline().strip()\n",
        "                self.print_progress_bar(i - self.start_idx, self.num_samples)\n",
        "                row = json.loads(line)\n",
        "\n",
        "                generated_code_data = self.process_line(line, row)\n",
        "                generated_codes.append(generated_code_data)\n",
        "\n",
        "                # Save progress to the output file after each iteration\n",
        "                self.save_output(generated_codes)\n",
        "\n",
        "                count = i - self.start_idx\n",
        "                if count + 1 >= self.num_samples:\n",
        "                    break\n",
        "            self.print_progress_bar(self.num_samples, self.num_samples)\n",
        "\n",
        "\n",
        "    def print_example_code(self, idx=0):\n",
        "\n",
        "        if os.path.exists(self.output_filename) and os.path.getsize(self.output_filename) > 0:\n",
        "            with open(self.output_filename, \"r\") as file:\n",
        "                generated_codes = json.load(file)\n",
        "                if generated_codes:\n",
        "                    # Take the first entry\n",
        "                    try:\n",
        "                      generated_code_data = generated_codes[idx]\n",
        "                    except IndexError:\n",
        "                      print(\"Index out of range.\")\n",
        "                      return\n",
        "\n",
        "                    task_id = generated_code_data.get(\"task_id\", \"No task available\")\n",
        "                    prompt = generated_code_data.get(\"instruction\", \"No prompt available\")\n",
        "                    code = generated_code_data.get(\"generated_code\", \"No code available\")\n",
        "\n",
        "                    print(\"Task \" + task_id + \":\")\n",
        "                    print(\"Example Prompt:\")\n",
        "                    print(prompt)\n",
        "                    print(\"\\nGenerated Code:\")\n",
        "                    print(code)\n",
        "                else:\n",
        "                    print(\"No data available in the output file.\")\n",
        "        else:\n",
        "            print(\"Output file does not exist or is empty.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NAsg1uwbhFs"
      },
      "outputs": [],
      "source": [
        "input_filename = \"humanevalpack.jsonl\"\n",
        "output_filename = \"output.json\"\n",
        "\n",
        "if os.path.exists(output_filename):\n",
        "  os.remove(output_filename)\n",
        "\n",
        "generator = RustCodeGenerator(\n",
        "    input_filename,\n",
        "    output_filename,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    # num_samples=5,\n",
        "    # start_idx=0,\n",
        "    max_new_tokens = 1024\n",
        ")\n",
        "\n",
        "generator.process()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  generator.print_example_code(i)\n",
        "  print(\"======================= NEXT ===========================\")"
      ],
      "metadata": {
        "id": "LrneGGskwqUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.print_example_code(89)"
      ],
      "metadata": {
        "id": "qaBm3UDUjnG9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}